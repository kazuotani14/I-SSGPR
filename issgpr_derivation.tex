\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\setlength{\parindent}{0pt}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[autostyle]{csquotes}  

% My packages
\usepackage{todonotes}
\usepackage{algorithm}% http://ctan.org/pkg/algorithm
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{float}% http://ctan.org/pkg/float
\usepackage{caption}

% \setcounter{section}{1} %start at section 2, to match handout

\title{Gaussian Process notes}
\author{Kazuya Otani}

\begin{document}
\maketitle

\section{Linear regression to GP regression to I-SSGPR}

Derivation and notes for "Real-time model learning using Incremental Sparse Spectrum Gaussian Process Regression", Arjan Gijsberts and Giorgio Metta, 2012.

Mostly just following the explanations in the paper, with additional calculation steps and commentary in between equations.

\subsection*{To-do}

\begin{itemize}
	\item automatic relevance detection
    \item Read rahimi and recht paper
    \item Read Locally Weighted Projection Regression paper
    \item Understand difference between this and other local GP. How do they compare in terms of complexity in updates, inference? What approximations do they make, and how do their expected errors compare?
    \item Finish some derivations 
\end{itemize}

\subsection{Linear regression}

Given examples $(x,y)$

Compute a linear model $\hat{Y}=Xw+b$ (reduced to $\hat{Y}=Xw$ by augmenting $x$ with constant term at end)... 

That minimizes $||Y - \hat{Y}||_2^2 = ||Y - X w||_2^2$. This amounts to Maximum Likelihood Estimation, where we have a Gaussian likelihood over $y$:

$$p(Y|X,w;\sigma^2) \propto \exp(-\frac{1}{2\sigma^2} ||Y-Xw||^2)$$

$$\nabla_w p(Y|X,w; \sigma^2) = -\frac{1}{\sigma^2} X^T(Y-Xw)=0$$

In the general case, $X$ is an $(m \times n)$ matrix with $m$ examples of $n$-dimensional inputs, and $Y$ is a vector of corresponding outputs.

\begin{align*}
& ||y - X w||_2^2 \\
&= (y-Xw)^T (y-Xw) \\
&= y^T y - 2(Xw)^T y + (Xw)^T(Xw)
\end{align*}

Taking the derivative of this with respect to $w$, we get:

\begin{align*}
-X^T Y + (X^T X)w = 0 \\
A = (X^T X)^{-1} X^T Y
\end{align*}

$(X^T X)^{-1} X^T$ is the Moore-Penrose pseudo-inverse. 

\subsection{Bayesian Linear Regression}

On top of linear regression, make two assumptions: 

\begin{itemize}
	\item Training examples $(x,y)$ come from a noisy underlying distribution $y=Xw + \epsilon$, where $\epsilon \sim N(0, \sigma_n^2)$
	\item Assume linear weights comes from a zero-mean Gaussian distribution $w \sim N(0, \Sigma_p)$ (prior)
\end{itemize}

This means $p(y | w, x)=N(x^T w, \sigma_n^2 I)$

Computing the posterior $w \sim N(\mu_n, \Sigma_n)$ for this, we get:

\begin{align*}
\mu_n &= (X^T X + \sigma_n^2 \Sigma_p^{-1})^{-1} X^T Y \\
\Sigma_n &= \sigma_n^2 (X^T X + \sigma_n^2 \Sigma_p^{-1})^{-1}
\end{align*}

This comes from Maximum A Posteori Estimation, with a Gaussian likelihood on $w$:

$$p_{BLR} = p_{LR} \cdot p_{\text{prior}} \propto \exp(\text{LR terms}) \exp(\text{prior terms}) = \exp(\text{LR terms}+\text{prior terms})$$

\begin{align*}
p(w|Y; X, \sigma^2) &\propto \exp \left(-\frac{1}{2\sigma^2} ||y-Xw||^2 - \frac{1}{2} w^T \Lambda w \right) \\
&= \exp \left(-\frac{1}{2\sigma^2} (y^T y - 2y^T X w + w^T X^T X w + \sigma^2 w^T \Lambda w) \right) \\
&= \exp \left(-\frac{1}{2\sigma^2} (w^T(X^TX + \sigma^2 \Lambda)w -2 y^T Xw + y^T) \right)
\end{align*}

\textbf{TODO}: Finish this derivation

\href{https://www.cs.utah.edu/~fletcher/cs6957/lectures/BayesianLinearRegression.pdf}{Reference link }

Note that this is equivalent to regularized linear regression. Regularization aka weight decay is the result of placing a zero-mean prior on the model weights! 

\subsection{Non-linear mappings}

To extend this framework to non-linear features, replace $X$ with $\Phi(X)$. For a prediction of $y$ at $x$: 

$$p(y | x, X, Y) = N(\phi(x)^T A^{-1} \phi(X)^T Y, \sigma_n^2 (1 +\phi(x)^T A^{-1} \phi(x)))$$
$$A = \phi(X)^T \phi(X) + \sigma_n^2 \Sigma_p^{-1}$$

Here, $A \in R^{n \times n}$. The most expensive step in inference is inverting $A$, which is $O(n^3)$. 

\subsection{Kernel trick}

The kernel trick makes use of the fact that the non-linear mapping $\phi(X)$ only appears in the inference equations as inner products: e.g. $\phi(x)^T A^{-1}\phi(x)$ and $\phi(X)^T \phi(X)$. 

We use a kernel / covariance function that can compute an inner product in closed form.

$$k(x_i, x_j) = (\Sigma^{1/2} \phi(x_i))^T (\Sigma^{1/2} \phi(x_i)) = \phi(x_i)^T \Sigma \phi(x_j)$$

\textbf{TODO}: Elaborate on kernel trick

Applying this trick to Bayesian Linear Regression leads Gaussian Process Regression.

\textbf{TODO}: Derive inference equations

$$p(y | x, X, Y) = N(k^T G^{-1}Y, k(x, x) - k^T G^{-1}k + \sigma_n^2)$$

$$K= [k(x_i, x_j)]_{i,j=1}^m, k = [k(x, x_i)]_{i=1}^m, G = K+\sigma_n^2 I$$ 

Here, $G \in R^{m \times m}$, and the most expensive step is in inverting $G$, which is $O(m^3)$. Notice that when $m>>n$, Gaussian Process Regression with the kernel trick is actually more expensive than directly projecting using $\phi(X)$! However, this assumes the projection is finite-dimensional, which some popular kernels (e.g. RBF kernel, aka squared exponential) are not. Because of this, we can't just directly compute $\phi(X)$.  

The RBF / squared exponential function is infinite-dimensional because in order to write it in a form $\phi(X)^T \phi(X)$ (inner product between two vectors, i.e. linear), we would have to take an infinitely long Taylor series. Details \href{https://math.stackexchange.com/questions/1606110/what-is-an-example-of-a-svm-kernel-where-one-implicitly-uses-an-infinity-dimens/1620256#1620256}{here}.

\subsection{Incremental updates}

Assume we have a finite-dimensional kernel function. In this case, if $m>>n$ (i.e. number of examples is much larger than dimensionality of training examples), it may be better to ditch the kernel function and compute the projections $\phi(X)$ directly. Here, we will derive an incremental method for computing the $A,b$ matrices

Recall:

$$p(y | x, X, Y) = N(\phi(x)^T A^{-1} \phi(X)^T Y, \sigma_n^2 (1 +\phi(x)^T A^{-1} \phi(x)))$$
$$A = \phi(X)^T \phi(X) + \sigma_n^2 \Sigma_p^{-1}$$

Here, $\sigma_n^2$ is the assumed noise in $y$, and $\Sigma_p$ is the covariance of the prior over our linear weights.

When a new sample $(x_t, y_t)$ comes in, we can decompose the update as follows:

\begin{align*}
\phi(X_t) &= \begin{bmatrix}\phi(X_{t-1})^T & \phi(x_t) \end{bmatrix} \\
Y_t &= \begin{bmatrix}Y_{t-1}^T & y_t \end{bmatrix}
\end{align*}

As shown above, to compute the posterior distribution we only need two values that are based on the training data:

$$A = \phi(X)^T \phi(X) + \sigma_n^2 \Sigma_p^{-1}, b = \phi(X)^T Y$$
$$p(y | x, X, Y) = N(\phi(x)^T A^{-1} b, \sigma_n^2 (1 +\phi(x)^T A^{-1} \phi(x)))$$

To update $A,b$ incrementally:

\begin{align*}
A_t &= \begin{bmatrix} \phi(X_{t-1}) & \phi(x_t) \end{bmatrix} \begin{bmatrix} \phi(X_{t-1} \\ \phi(x_t) \end{bmatrix} + \sigma_n^2 \Sigma_p^{-1} \\
&= \phi(X_{t-1})^T\phi(X_{t-1}) + \phi(x_t)^T \phi(x_t) + \sigma_n^2 \Sigma_p^{-1} \\
&= A_{t-1} + \phi(x_t)^T \phi(x_t)
\end{align*}

\begin{align*}
b_t &= \phi(X_t)^T Y_t  \\
&= \begin{bmatrix} \phi(X_{t-1}) & \phi(x_t)) \end{bmatrix} \begin{bmatrix} Y_{t-1}\\ y_t \end{bmatrix} \\
&- \phi(X_{t-1})^T Y_{t-1} + \phi(x_t)^T y_t \\
&= b_{t-1} + \phi(x_t)^T y_t
\end{align*}

Thus, the updates to both $A$ and $b$ are rank-one updates. Set $A_0 = \sigma_n^2 \Sigma_p^{-1}, b_0=0$. 

However, this doesn't make training any more efficient because the problem of computing $A^{-1}$ with $O(n^3)$ complexity remains. The solution to this is to update the Cholesky factor of $A$ instead, since this can be used to compute an "inverse" without actually inverting the matrix. 


\subsubsection*{"Inverting" a matrix with Cholesky factorization}

Most of the time when we invert a matrix, it's for the purpose of solving a linear equation, i.e. multiplying that inverse with a vector. 

$$Ax=b \rightarrow x = A^{-1}b$$

Cholesky factorization gives us a positive definite matrix as an inner product of an upper triangular (in this case; lower-triangular also used) matrix $R\in (n \times n)$: 

$$A=R^T R$$

The trick for using Cholesky factorization to "invert" matrices is to notice that $Rx=b$ is actually solvable without explicitly inverting the matrix - since $R$ is upper triangular, the bottom row defines a simple equation $x_j=b_j/R_{jj}$. This can then be substituted into the second row from the bottom with substitution. So solving $Rx=b$, then $R^T z = b$ is equivalent to computing $(R^T R)^{-1}b$. 

Given this factorization, we can multiply the inverse using forward and backward substitution. This makes the "inverse" $O(n^2)$.

\begin{align*}
A = R^T R \rightarrow A^{-1} &= R^{-1}R^{-T} \\
A^{-1} v \rightarrow R^{-1}R^{-T} v
\end{align*}

\subsubsection*{Incremental Cholesky updates}

\begin{align*}
A &=R^T R \\
A_t &= R_t^T R_t \\
&= A_{t-1} + \phi(x_t)^T \phi(x_t) \\
&= R_{t-1}^T R_{t-1} + \phi(x_t)^T \phi(x_t) \\ 
% R_t^T R_t &= \tilde{R}_t^T \tilde{R}_t \\
\tilde{R}_t &= \begin{bmatrix}R_{t-1} & \phi(x_t) \end{bmatrix}
\end{align*}

We want to transform: 

$$R_t^T R_t = \begin{bmatrix} R_{t-1} & \phi(x_t) \end{bmatrix}^T \begin{bmatrix} R_{t-1} \\ \phi(x_t) \end{bmatrix} \rightarrow \begin{bmatrix} R_{t} & 0 \end{bmatrix}^T \begin{bmatrix} R_{t} \\ 0 \end{bmatrix}$$

We can do this by applying an orthogonal matrix/transformation, like the ones used for QR factorization: Givens rotations or Householder reflections. For orthogonal matrices: $Q^{-1}=Q^T, Q^T Q = I$.

\begin{align*}
R^T R &= (Q \begin{bmatrix} R_{t} & 0 \end{bmatrix})^T (Q \begin{bmatrix} R_{t} \\ 0 \end{bmatrix}) \\
&= \begin{bmatrix} R_{t} & 0 \end{bmatrix} Q^T Q_t \begin{bmatrix} R_{t} \\ 0 \end{bmatrix} \\
&= \begin{bmatrix} R_{t} & 0 \end{bmatrix} \begin{bmatrix} R_{t} \\ 0 \end{bmatrix}
\end{align*}

\subsubsection*{Givens rotations}

\href{https://en.wikipedia.org/wiki/Givens_rotation}{Givens rotations} are a method used to zero out subdiagonal elements, commonly employed in QR factorization. Main idea is that it multiplies two rows of a matrix by a rotation matrix (embedded inside an identity matrix), to zero out a single element in a column. Note that although Givens rotations are usually used to zero out subdiagonals, in this use case they are used to zero out an entire row, one element at a time. \href{http://drsfenner.org/blog/2016/03/givens-rotations-and-qr/}{Reference link}

The main intuitive difference between Givens rotations and Householder reflections (another method commonly used for QR) is that Givens rotations zero out one element at a time while affecting only two rows at a time, while Householder reflections zero out entire subdiagonals on a column while affecting the entire matrix. On a dense matrix, Householder reflections are more efficient (by about 50\%). Givens rotations can be preferable for their parallelizability, or for sparse matrices (which have fewer elements to zero out).  

\textbf{TODO}: Figure out how exactly to zero out entire row with Givens rotations. My current implementation just uses built-in QR factorization.

\subsection{Results}

By incrementally updating the Cholesky factor of $A$, we get the most expensive step down to complexity $O(n^2)$, where $n$ is dimensionality of non-linear mapping. This means we can compute an exact Gaussian Process Regression in $O(mn^2)$! But this is assuming that we have a finite-dimensional kernel, which is not the case for the most popular kernel function: RBF / squared exponential:

$$k(x_i, x_j) = \sigma_f^2 \exp(-\frac{1}{2}(x_i-x_j)^T M (x_i-x_j))$$

where $M$ is a diagonal matrix with $M_{ii}=l_i^{-2}$ (length scales). We can deal with this with a finite-dimensional approximation.

\subsection{Approximating RBF kernel}

"The feature mapping for the popular RBF kernel is infinite-dimensional and thus cannot be explicitly computed. Rahimi and Recht
(2008a), however, demonstrate that the RBF and other shift 
invariant kernels $k(x_i, x_j) = k(x_i âˆ’ x_j)$ can be approximated to an arbitrary precision using a finite dimensional random feature mapping."

This random feature mapping is done in the spectral (frequency) domain: 

$$k(x_i - x_j) = E_\omega\begin{bmatrix}z_\omega(x_i) & z_\omega(x_j) \end{bmatrix}$$
$$z_\omega(x) = \begin{bmatrix}\cos(\omega^T x) & \sin(\omega^T x)  \end{bmatrix}$$

$\omega$ is drawn according to some probability distribution $\mu$ that is derived with the inverse Fourier transform (not sure how this works)

We can get an approximation by averaging over $D$ projections:

$$\phi(x) = \frac{1}{\sqrt{D}} \begin{bmatrix}z_{\omega_1}(x) & \ldots z_{\omega_D}(x) \end{bmatrix}$$

The random feature mapping $\phi(X)$ that corresponds to the RBF kernel is shown in the paper. Using this mapping, we can perform incremental updates with complexity $O(D^2)$, where $D$ is a parameter that trades off between approximation accuracy and computation speed.

\textbf{TODO}: Put phi equation for rbf here. How are length scales computed?

\subsection*{Comparison to other GP extensions}

Two methods cited at bottom of Black-DROPS paper:

\begin{itemize}
	\item "Patchwork Kriging for Large-scale Gaussian Process Regression"
    
    Partition input domain into multiple regions, with different local GP fitted to each region. Enforce continuity between regions with pseudo-observations at borders. 
    
    Complexity is $O(KM^3)$, for updating $K$ local GPs of dimensions $M$ each. 
    
    Need to read paper. For inference/training, it seems like each sample updates every local GP. For prediction, do we only use the local GP? 
    
    \item "Distributed Gaussian Processes", Deisenroth and Ng 
    
    Not sure what they're doing here; looks complicated. Basic idea seems to be decomposition/parallelization of GP solve. Do they make approximations? 

\end{itemize}

Local GPs are sparsification/parallelization.

Note that both of these methods do batch training, instead of incremental. It seems intuitive that incremental updates would be more efficient, if a formulation is possible. 

EPC uses Locally Weighted Projection Regression - why? They note that LWPR is $O(dim(z))$, where $z$ is input dimensionality?? 

The I-SSGPR paper shows that it is both faster and learns faster/better than LWPR. 

"Although LWPR can more accurately represent accumulated experiences via its local models, we also consider ISSGPR due to its potential for faster model adaptation."

Simple sparsification scheme that works well in practice, from Limbo: \href{https://github.com/resibots/limbo/blob/master/src/limbo/model/sparsified_gp.hpp}{sparsified_gp} (\href{notes are from here}{https://github.com/resibots/limbo/pull/254}. Basic idea is to set a maximum number of training samples, then selectively remove samples when that maximum number is exceeded. Samples are removed in order of "density", which can be measured as the sum of the euclidean distances between a sample and its $D$ closest neighbors. We assume that functions are smoothed and can be modeled by evenly spaced points. Note: this method does not take into account variation in observations. 

\section{Misc notes}

\subsection{Automatic Relevance Detection}

Automatic relevance detection refers to kernels that have (hyper) parameters in them that can be optimized to maximize a metric of model fit. The most commonly used one is the squared exponential kernel with automatic relevance detection (see Rasmussen chapter 5). 

$$k(x_p, x_q) = \sigma_f^2 \exp(-\frac{1}{2} (x_p - x_q)^T M (x_p - x_q) ) + \sigma_n^2 \delta_{pq} \\$$
$$M_1 = \ell^{-2} I, M_2 = \text{diag}(\mathbf{\ell})^{-2}, M_3 = \Lambda \Lambda^T + \text{diag}(\ell)^{-2}, \hspace{0.5mm} \text{where} \hspace{1mm} \Lambda \in R^{D\times k}, k<D$$

$M_1, M_2, M_3$ are options with increasing expressiveness/adjustability. For equations below we'll just call all optimizable parameters $\theta$. 

We want to maximize the probability of data $y$ given hyperparameters $\theta$: 

$$ \log p(y | X, \theta) = -\frac{1}{2} y^T K_y ^{-1} y - \frac{1}{2} \log |K_y| \frac{n}{2} \log 2\pi$$

\textbf{TODO where this equation comes from}

First term is data fit (note that this is the only term with $y$ in it), second term is a complexity penalty, third term is normalization constant. 

We can take the derivative of this with respect to $\theta$: 

\begin{align*}
\frac{\partial}{\partial \theta} \log p(y | X, \theta) &= \frac{1}{2} y^T K^{-1} \frac{\partial K}{\partial \theta} K^{-1}y - \frac{1}{2} \text{tr}(K^{-1} \frac{\partial K}{\partial \theta}) \\
&= \frac{1}{2} \text{tr}\left( (\alpha \alpha^T - K^{-1})  \frac{\partial K}{\partial \theta} \right), \alpha = K^{-1}y
\end{align*}

\textbf{TODO figure out how to get to second line here}

Using this derivative, we can just run any gradient-based optimization algorithm (e.g. SGD, RProp). Local minima do exist, but in practice are not too much of a problem, and each minimum corresponds to a specific fit-complexity tradeoff.

$\text{tr}(A)=\sum_{i=1}^n a_{ii}$ - the sum of elements on main diagonal on a square matrix. This also corresponds to the sum of the eigenvalues!

Some matrix derivatives used for this: 

\begin{align*}
\frac{\partial}{\partial \theta} K^{-1} &= -K^{-1} \frac{\partial K}{\partial \theta} K^{-1} \\
\frac{\partial}{\partial \theta} \log |K| &= \text{tr}(K^{-1} \frac{\partial K}{\partial \theta})
\end{align*}

Derivative of the kernel matrix $K$ is pretty straightforward. Example for simple squared exponential below: 

\begin{align*}
K(x_p, x_q) &= \sigma_f^2 \exp(-\frac{1}{2} (x_p - x_q)^T \text{diag}(\mathbf{\ell})^{-2} (x_p - x_q) ) \\
\frac{\partial K}{\partial \sigma_f} &= 2 \sigma_f \exp\left(-\frac{1}{2} (x_p - x_q)^T \text{diag}(\mathbf{\ell})^{-2} (x_p - x_q) \right) \\
\frac{\partial K}{\partial \ell} &= - \frac{1}{2} \sigma_f^2 (x_p - x_q)^T \text{diag}(\mathbf{\ell})^{-2} (x_p - x_q) \exp\left(-\frac{1}{2} (x_p - x_q)^T \text{diag}(\mathbf{\ell})^{-2} (x_p - x_q) \right)
\end{align*}

\subsubsection*{Calculating logdet term}

Cool trick for calculating the $\log |K_y|$ term in the likelihood, from Limbo. $\text{logdet}(A)$ is often more numerically stable than $\log \text{det} (A)$. 

Using these properties: $A = LL^T$, $\text{det}(L) = \prod_i L_{ii}$ (determinant of lower triangular matrix), $\log \prod_i x_i = \sum \log x_i$ (log rule)

$$A = LL^T \rightarrow \text{logdet}(A) = 2 \sum \log \text{diag}(L)$$

% \begin{minipage}{\linewidth}
%   \centering
%   \includegraphics[width=6cm]{figures/figure_name.png}
%   \captionof{figure}{Caption}
% \end{minipage}

\section{References}

\begin{itemize}
	\item \href{http://www.gaussianprocess.org/gpml/}{Gaussian Processes for Machine Learning (book), Rasmussen and Williams}
	\item https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA 
	\item Machine learning summer school: lectures on GPs. \href{https://www.video.ethz.ch/conferences/2014/rnls/05_thursday/0f8953ea-b1fe-4960-8fa1-ef71bb642445.html}{Video 1} %, \href{Video 2}{https://www.youtube.com/watch?v=50Vgw11qn0o} 
	\item More lectures: https://www.youtube.com/watch?v=vz3D36VXefI
	\item \href{https://github.com/resibots/limbo}{Limbo}: library for GPs
\end{itemize}

Papers:

\begin{itemize}
	\item Gijsberts, Arjan, and Giorgio Metta. "Real-time model learning using incremental sparse spectrum gaussian process regression." Neural Networks 41 (2013): 59-69.
	\item Chatzilygeroudis, Konstantinos, et al. "Black-box data-efficient policy search for robotics." Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017.
	\item https://sites.google.com/site/dataefficientml/home
\end{itemize}

Tutorials

\begin{itemize}
	\item http://willwolf.io/2018/03/31/gaussian-algebra-to-gaussian-processes-part-1/
	\item https://gist.github.com/Bridgo/429594942ff51037ecc703905e40c562
	\item http://bridg.land/posts/gaussian-processes-1
	\item http://efavdb.com/gaussian-processes/
\end{itemize}

% \todo[inline]{notes}

% \begin{thebibliography}{1}
% \bibitem{simbicon} Yin, KangKang, Kevin Loken, and Michiel van de Panne. "Simbicon: Simple biped locomotion control." ACM Transactions on Graphics (TOG). Vol. 26. No. 3. ACM, 2007.
% \end{thebibliography}

\end{document}